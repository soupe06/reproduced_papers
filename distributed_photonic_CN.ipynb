{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9037d8",
   "metadata": {},
   "source": [
    "# Distributed Quantum Neural Networks in Merlin (arXiv:2505.08474v1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a81ac7",
   "metadata": {},
   "source": [
    "## Training a conventional CNN on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39808ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:01:46.871596Z",
     "start_time": "2025-08-21T20:01:44.096961Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ArnoRicou\\Customer_projects\\Tests\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Importing ML libraries\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "#Quandela libraries\n",
    "import perceval as pcvl\n",
    "from merlin import PhotonicBackend, CircuitType, StatePattern, AnsatzFactory, QuantumLayer\n",
    "from merlin import OutputMappingStrategy\n",
    "import merlin as ML\n",
    "\n",
    "#Importing local libraries\n",
    "\n",
    "from boson_sampler import BosonSampler\n",
    "from utils import MNIST_partial, accuracy, plot_training_metrics\n",
    "\n",
    "#Importing system libraries\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "#Importing other libraries\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9644fb3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:01:47.028578Z",
     "start_time": "2025-08-21T20:01:46.876770Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load the datas\n",
    "\n",
    "# dataset from csv file, to use for the challenge\n",
    "train_dataset = MNIST_partial(split = 'train')\n",
    "val_dataset = MNIST_partial(split='val')\n",
    "\n",
    "# definition of the dataloader, to process the data in the model\n",
    "# here, we need a batch size of 1 to use the boson sampler\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc85877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:01:49.226729Z",
     "start_time": "2025-08-21T20:01:49.223776Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=4)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(4, 4, kernel_size=4)\n",
    "        self.fc1 = nn.Linear(4*4*4, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f986962e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:01:51.116997Z",
     "start_time": "2025-08-21T20:01:51.111179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters in classical CNN model:  978\n"
     ]
    }
   ],
   "source": [
    "#Define training hyperparameters\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 3\n",
    "\n",
    "\n",
    "# Instantiate the model and loss function\n",
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Count number of parameters in the model\n",
    "num_classical_parameter = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# of parameters in classical CNN model: \", num_classical_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11f2239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:54:36.403282Z",
     "start_time": "2025-08-21T19:54:33.255428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928d1093",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:54:36.533268Z",
     "start_time": "2025-08-21T19:54:36.463415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 82.33%\n"
     ]
    }
   ],
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_test_list = [] \n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        loss_test = criterion(outputs, labels).cpu().detach().numpy()\n",
    "        loss_test_list.append(loss_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a7c62",
   "metadata": {},
   "source": [
    "## Reducing the number of parameters to train the CNN by using a photonic quantum neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff177fc",
   "metadata": {},
   "source": [
    "Add Figure 2 of paper, with simpler description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b51286f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:01:58.908706Z",
     "start_time": "2025-08-21T20:01:58.756564Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "#Load the datas\n",
    "\"\"\"transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\"\"\"\n",
    "\n",
    "train_dataset = MNIST_partial(split='train')\n",
    "val_dataset = MNIST_partial(split='val')\n",
    "\n",
    "# definition of the dataloader, to process the data in the model\n",
    "# here, we need a batch size of 1 to use the boson sampler\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de6cd41d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:01:59.426133Z",
     "start_time": "2025-08-21T20:01:59.422460Z"
    }
   },
   "outputs": [],
   "source": [
    "#The size of the PQNN (photonic quantum neural network) will depend on the number parameters needed in the classical CNN\n",
    "#We know that a PQCNN of n photons with m modes covers m!/(n!*(m-n)!) outcomes.\n",
    "#We want num_classical_parameters in the CNN to be lower than number of outcomes in the PQCNN\n",
    "#In our case we take two PQCNNs so the total number of outcomes, is the product of the number of outcome in each of them : total_pqcnn_params = number_PQCNN1_outcomes * number_PQCNN2_outcomes\n",
    "\n",
    "\n",
    "#TODO : \n",
    "#- Define a function that takes the number of CNN params and number of PQCNN wanted as the input, and output the number of photons and modes for each PQCNN\n",
    "#- Hyperparams optimization : additional layers, like in original repo ? Learning  rate scheduler ?\n",
    "#- Add MPS layer to reduce number of params\n",
    "\n",
    "\n",
    "\n",
    "#In the meantime we hardcode the numbers of photons and modes for that specific case\n",
    "\n",
    "#n_1 represents the number of photons in the first PQCNN\n",
    "n_1 = 4\n",
    "m_1 = 9\n",
    "\n",
    "n_2 = 4\n",
    "m_2 = 8\n",
    "\n",
    "initial_state_1 = [1]*n_1 + [0] * (m_1 - n_1)\n",
    "initial_state_2 = [1]*n_2 + [0] * (m_2 - n_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4519ff24ff1d684a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:10:03.615928Z",
     "start_time": "2025-08-21T20:10:03.607621Z"
    }
   },
   "outputs": [],
   "source": [
    "class QuantumParameterizedCNN(nn.Module):\n",
    "    def __init__(self, num_classical_parameter):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = CNNModel()\n",
    "\n",
    "        PQCNN_1 = pcvl.GenericInterferometer(m_1,\n",
    "                                lambda i: pcvl.BS() // pcvl.PS(pcvl.P(f\"theta_1i{i}_ps\")) // \\\n",
    "                                            pcvl.BS() // pcvl.PS(pcvl.P(f\"theta_1o{i}_ps\")),\n",
    "                                shape=pcvl.InterferometerShape.RECTANGLE)\n",
    "\n",
    "        PQCNN_2 = pcvl.GenericInterferometer(m_2,\n",
    "                                lambda i: pcvl.BS() // pcvl.PS(pcvl.P(f\"theta_2i{i}_ps\")) // \\\n",
    "                                            pcvl.BS() // pcvl.PS(pcvl.P(f\"theta_2o{i}_ps\")),\n",
    "                                shape=pcvl.InterferometerShape.RECTANGLE)\n",
    "\n",
    "\n",
    "        self.QL_1 = ML.QuantumLayer(\n",
    "                    input_size=None,\n",
    "                    output_size=None,\n",
    "                    circuit=PQCNN_1,\n",
    "                    trainable_parameters=[\"theta\"],\n",
    "                    input_parameters=[],\n",
    "                    input_state= initial_state_1,\n",
    "                    no_bunching=False,\n",
    "                    output_mapping_strategy=OutputMappingStrategy.NONE,\n",
    "                )\n",
    "\n",
    "        self.QL_2 = ML.QuantumLayer(\n",
    "                    input_size=None,\n",
    "                    output_size=None,\n",
    "                    circuit=PQCNN_2,\n",
    "                    trainable_parameters=[\"theta\"],\n",
    "                    input_parameters=[],\n",
    "                    input_state= initial_state_2,\n",
    "                    no_bunching=False,\n",
    "                    output_mapping_strategy=OutputMappingStrategy.NONE,\n",
    "                )\n",
    "\n",
    "        self.num_classical_parameter = num_classical_parameter\n",
    "\n",
    "        self.register_buffer('param_shapes', torch.tensor([p.numel() for p in self.cnn.parameters()]))\n",
    "\n",
    "        # Store a template state dict for reference\n",
    "        self.template_state_dict = self.cnn.state_dict()\n",
    "\n",
    "    def probs_to_weights(self, probs_):\n",
    "        \"\"\"\n",
    "        Convert probability tensor to CNN weights + preserves gradient\n",
    "        \"\"\"\n",
    "        weight_dict = {}\n",
    "        data_iterator = probs_.view(-1)\n",
    "\n",
    "        for name, param in self.template_state_dict.items():\n",
    "            shape = param.shape\n",
    "            num_elements = param.numel()\n",
    "\n",
    "            if len(data_iterator) < num_elements:\n",
    "                chunk = torch.cat([\n",
    "                    data_iterator,\n",
    "                    torch.zeros(num_elements - len(data_iterator), device=data_iterator.device)\n",
    "                ])[:num_elements].reshape(shape)\n",
    "            else:\n",
    "                chunk = data_iterator[:num_elements].reshape(shape)\n",
    "\n",
    "            weight_dict[name] = chunk  # to preserve the gradient flow\n",
    "            data_iterator = data_iterator[num_elements:]\n",
    "\n",
    "        return weight_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get probabilities from quantum layers\n",
    "        probs1 = self.QL_1()\n",
    "        probs2 = self.QL_2()\n",
    "\n",
    "        tensor_product = torch.outer(probs1, probs2).flatten()\n",
    "\n",
    "        if tensor_product.size(0) < self.num_classical_parameter:\n",
    "            padded_tensor = torch.cat([\n",
    "                tensor_product,\n",
    "                torch.zeros(self.num_classical_parameter - tensor_product.size(0), device=x.device)\n",
    "            ])\n",
    "        else:\n",
    "            padded_tensor = tensor_product[:self.num_classical_parameter]\n",
    "\n",
    "        # Convert probabilities to real-valued weights\n",
    "        eps = 1e-6\n",
    "        tensor_product_clipped = padded_tensor.clamp(eps, 1 - eps)\n",
    "        new_params = torch.log(tensor_product_clipped / (1 - tensor_product_clipped))\n",
    "        new_params = torch.tanh(new_params)  # Optional scaling\n",
    "\n",
    "        # Convert to CNN weights + preserve gradients\n",
    "        weight_dict = self.probs_to_weights(new_params)\n",
    "\n",
    "        # FUNCTIONAL forward pass (preserves gradients)\n",
    "        return self.forward_functional(x, weight_dict)\n",
    "\n",
    "    def forward_functional(self, x, weight_dict):\n",
    "        \"\"\"\n",
    "        Functional forward pass that preserves gradients to quantum layers\n",
    "        \"\"\"\n",
    "        \n",
    "        # Conv1 + Pool\n",
    "        x = F.conv2d(x, weight_dict['conv1.weight'], weight_dict.get('conv1.bias', None))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        # Conv2 + Pool\n",
    "        x = F.conv2d(x, weight_dict['conv2.weight'], weight_dict.get('conv2.bias', None))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        # Flatten + FC\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.linear(x, weight_dict['fc1.weight'], weight_dict.get('fc1.bias', None))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "259f03e1e6715c14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:11:55.693200Z",
     "start_time": "2025-08-21T20:11:55.657543Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Instantiate the model\n",
    "#qmodel = HQCNNModel_v2()\n",
    "qmodel = QuantumParameterizedCNN(num_classical_parameter)\n",
    "\n",
    "# Instantiate loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(qmodel.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7566fa5301715135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:12:52.267469Z",
     "start_time": "2025-08-21T20:11:55.822369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/375], Loss: 2.3260\n",
      "Epoch [1/10], Step [200/375], Loss: 2.3112\n",
      "Epoch [1/10], Step [300/375], Loss: 2.2987\n",
      "Epoch [1/10] completed - Train ACC: 0.1288\n",
      "Epoch [2/10], Step [100/375], Loss: 2.1961\n",
      "Epoch [2/10], Step [200/375], Loss: 2.3373\n",
      "Epoch [2/10], Step [300/375], Loss: 2.3675\n",
      "Epoch [2/10] completed - Train ACC: 0.1357\n",
      "Epoch [3/10], Step [100/375], Loss: 2.3440\n",
      "Epoch [3/10], Step [200/375], Loss: 2.3429\n",
      "Epoch [3/10], Step [300/375], Loss: 2.2669\n",
      "Epoch [3/10] completed - Train ACC: 0.1448\n",
      "Epoch [4/10], Step [100/375], Loss: 2.3472\n",
      "Epoch [4/10], Step [200/375], Loss: 2.2338\n",
      "Epoch [4/10], Step [300/375], Loss: 2.3031\n",
      "Epoch [4/10] completed - Train ACC: 0.1415\n",
      "Epoch [5/10], Step [100/375], Loss: 2.3008\n",
      "Epoch [5/10], Step [200/375], Loss: 2.2822\n",
      "Epoch [5/10], Step [300/375], Loss: 2.3310\n",
      "Epoch [5/10] completed - Train ACC: 0.1393\n",
      "Epoch [6/10], Step [100/375], Loss: 2.2549\n",
      "Epoch [6/10], Step [200/375], Loss: 2.1543\n",
      "Epoch [6/10], Step [300/375], Loss: 2.2807\n",
      "Epoch [6/10] completed - Train ACC: 0.1537\n",
      "Epoch [7/10], Step [100/375], Loss: 2.2811\n",
      "Epoch [7/10], Step [200/375], Loss: 2.2575\n",
      "Epoch [7/10], Step [300/375], Loss: 2.3612\n",
      "Epoch [7/10] completed - Train ACC: 0.1437\n",
      "Epoch [8/10], Step [100/375], Loss: 2.2759\n",
      "Epoch [8/10], Step [200/375], Loss: 2.2745\n",
      "Epoch [8/10], Step [300/375], Loss: 2.1891\n",
      "Epoch [8/10] completed - Train ACC: 0.1423\n",
      "Epoch [9/10], Step [100/375], Loss: 2.2330\n",
      "Epoch [9/10], Step [200/375], Loss: 2.2151\n",
      "Epoch [9/10], Step [300/375], Loss: 2.3487\n",
      "Epoch [9/10] completed - Train ACC: 0.1573\n",
      "Epoch [10/10], Step [100/375], Loss: 2.1695\n",
      "Epoch [10/10], Step [200/375], Loss: 2.2317\n",
      "Epoch [10/10], Step [300/375], Loss: 2.3026\n",
      "Epoch [10/10] completed - Train ACC: 0.1418\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# Fixed: proper accuracy calculation and gradient clipping\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0  # Reset for each epoch\n",
    "    total = 0    # Reset for each epoch\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        qmodel.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = qmodel(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        ql1_grads = [p.grad for p in qmodel.parameters() if p.requires_grad]\n",
    "        #print(ql1_grads)\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        max_norm = 1.0\n",
    "        utils.clip_grad_norm_(qmodel.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate training accuracy for this batch\n",
    "        with torch.no_grad():\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print epoch accuracy\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed - Train ACC: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "379d5ee9f6275da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:13:39.246559Z",
     "start_time": "2025-08-21T20:13:39.033224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 8.83%\n"
     ]
    }
   ],
   "source": [
    "# Testing loop\n",
    "qmodel.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_test_list = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = qmodel(images)\n",
    "        loss_test = criterion(outputs, labels).cpu().detach().numpy()\n",
    "        loss_test_list.append(loss_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d50c1de291fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
